{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook showcases some data analysis done on a text file downloaded from the Web and provides a set of practice exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project Gutenberg's Alice's Adventures in Wonderland, by Lewis Carroll\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Alice's Adventures in Wonderland\r\n",
      "\r\n",
      "Author: Lewis Carroll\r\n",
      "\r\n",
      "Posting Date: June 25, 2008 [EBook #11]\r\n",
      "Release Date: March, 1994\r\n",
      "[Last updated: December 20, 2011\n"
     ]
    }
   ],
   "source": [
    "#Get a text file.\n",
    "#Get book \"Alice's Adventures in Wonderland\" from Project Gutenberg, in text format\n",
    "#Store text content to a file named alice.txt\n",
    "import requests\n",
    "import codecs # Force utf8 as the character encoding\n",
    "\n",
    "url='http://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "text_page = requests.get(url).text\n",
    "with codecs.open(\"alice.txt\", \"w\", \"utf8\") as file:\n",
    "    file.write(text_page)\n",
    "#Look at the text_page object\n",
    "#?text_page\n",
    "\n",
    "#Look at the first 500 characters of the book\n",
    "print(text_page[:500])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chapters:  12\n",
      "Chapter lengths:  [11453, 10989, 9552, 13871, 11986, 13860, 12688, 13656, 12618, 11528, 10380, 30430]\n",
      "Min chapter length:  9552\n",
      "Max chapter length:  30430\n",
      "Avg chapter length:  13584.25\n",
      "Numpy Avg chapter length:  13584.25\n",
      "Deviation from avg chapter length:  5246.15718288158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Open the file where we stored the book \"Alice's Adventures in Wonderland\"\n",
    "#Load data into object text\n",
    "with open('alice.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "#Example of Descriptive Analysis\n",
    "#How many chapters does the book have?\n",
    "#Ignore the first element at index 0 which is the pre-face, \n",
    "#not a chapter. See book structure.\n",
    "chapters = text.split(\"CHAPTER\")[1:]\n",
    "chapter_lengths = [len(ch) for ch in chapters]\n",
    "\n",
    "min_chapter_length = len(min(chapters, key=len))\n",
    "max_chapter_length = len(max(chapters, key=len))\n",
    "\n",
    "avg_chapter_length = sum(chapter_lengths)/len(chapters)\n",
    "#Can use numpy for numeric computations such as mean, standard deviation\n",
    "numpy_avg_chapter_length = np.mean(chapter_lengths)\n",
    "stdev_chapter_length = np.std(chapter_lengths)\n",
    "\n",
    "#?nchapters\n",
    "print(\"Number of chapters: \", len(chapters))\n",
    "print(\"Chapter lengths: \", chapter_lengths)\n",
    "print(\"Min chapter length: \", min_chapter_length)\n",
    "print(\"Max chapter length: \", max_chapter_length)\n",
    "print(\"Avg chapter length: \", avg_chapter_length)\n",
    "print(\"Numpy Avg chapter length: \", numpy_avg_chapter_length)\n",
    "print(\"Deviation from avg chapter length: \", stdev_chapter_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5a6969fe175c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#To download ready-to-use nltk datasets run in your shell: python -m nltk.downloader all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#You need the nltk.corpus.stopwords.words('english') for this code to run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#matplotlib is for plotting and nice visualisations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#numpy is a package for numeric computing\n",
    "import numpy as np\n",
    "#nltk is a package for working with text\n",
    "#Also see: http://www.nltk.org/book/\n",
    "#To download ready-to-use nltk datasets run in your shell: python -m nltk.downloader all\n",
    "#You need the nltk.corpus.stopwords.words('english') for this code to run\n",
    "import nltk\n",
    "#matplotlib is for plotting and nice visualisations\n",
    "import matplotlib\n",
    "#Package re is for regualr expressions\n",
    "import re # functions fior dealing with regular expressions\n",
    "\n",
    "#Open the file where we stored the book \"Alice's Adventures in Wonderland\"\n",
    "#Load data into object text\n",
    "with open('alice.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "#Example of Exploratory Analysis\n",
    "#Where in the text do given words occur? (visualisation)\n",
    "#What are the top-20 most frequent words in this book? (summarization)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "words = nltk.word_tokenize(text)\n",
    "#print(words[:10])\n",
    "print(\"Number of sentences: \", len(sentences))\n",
    "print(\"Number of words: \", len(words))\n",
    "\n",
    "#Turn the text into an nltk object Text\n",
    "alice = nltk.Text(words)\n",
    "#?alice\n",
    "#Where in the text do given words occur? (visualisation)\n",
    "alice.dispersion_plot([\"Alice\", \"Rabbit\", \"Duchess\"])\n",
    "\n",
    "# Remove all punctuation from word lists - note the use of regular expressions!\n",
    "#\\W Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
    "#For more details: https://docs.python.org/3/howto/regex.html\n",
    "alice = [w for w in alice if not (re.match(r'^\\W+$', w) != None)]\n",
    "\n",
    "# Convert all words to lower case\n",
    "alice = [w.lower() for w in alice]\n",
    "\n",
    "# Remove all stop words from word lists\n",
    "alice = [w for w in alice if not w in nltk.corpus.stopwords.words('english')]\n",
    "#print(alice[:10])\n",
    "\n",
    "word_frequency = nltk.FreqDist(alice)\n",
    "# #?word_frequency\n",
    "#What are the top-10 most frequent words in this book?\n",
    "print(\"\\nMost frequent top-10 words: \", word_frequency.most_common(10))\n",
    "word_frequency.plot(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Practice downloading files from the Internet, doing some preliminary data analysis and extracting new knowledge from them. Below you are given a set of exercises requesting you to download PDF files, extract their text content and do some analysis using the text. You can do this using other types of data of your own interest (e.g., news articles in HTML format). We work with PDF here because many reports are released in PDF format (e.g., http://www.publicpolicy.ie/download-center/downloads-by-theme/) and it is useful to know how to extract their content and work with it with the aim to extract new knowledge.\n",
    "\n",
    "## Problem Setup\n",
    "In February 2016, the Irish General Elections took place. Many of the participating political parties released their political program (aka manifestos) ahead of the election, in PDF formats. At the URL below you can find a sample of such reports. \n",
    "Please do the following:\n",
    "\n",
    "1. For two Irish political parties of your own choice, search and download the files containing their General Elections 2016 political manifesto (i.e., political plan). A few manifestos available online in pdf format are given at this link: [GE16 Party Manifestos](https://drive.google.com/drive/folders/0B0F5ayfFxHc7Y2xBOE5uY1VzSFE?usp=sharing)\n",
    "    \n",
    "2. Extract the text content from the two files, and save it to two different text files on your computer.\n",
    "\n",
    "3. Using the text of each political manifesto, compute and print descriptive statistics: \n",
    "    - print the first 500 characters of the text, \n",
    "    - print the total number of sentences and the shortest and the longest sentence, \n",
    "    - print the total number of words and the ratio of unique words to total number of words, \n",
    "    - print the most frequent 20 words.\n",
    "    \n",
    "4. For each text file containing the political manifesto:\n",
    "    - remove the punctuation from the text, transform all words to lower case and remove the stop words,\n",
    "    - print the first 500 characters of the cleaned text,\n",
    "    - print the top-20 most frequent words in the cleaned text, \n",
    "    - on the cleaned text, use a dispersion plot to show the coverage of different issues in the two manifestos, where the issues are: \"abortion\", \"crime\", \"debt\", \"economy\", \"environment\", \"health\", \"housing\", \"infrastructure\" , \"jobs\", \"tax\".\n",
    "\n",
    "5. Choose two different methods of summarising and contrasting the two political manifestos to give a better understanding of the characteristics and focus of each. Discuss the advantages and disadvantages of the methods proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
